import streamlit as st
from rag_extract_variables import search_chunks, extract_variables_from_chunks

st.set_page_config(page_title="ğŸ“ Research Variable Extractor", layout="wide")
st.title("ğŸ” Academic Assistant â€“ Variable Extraction")

# ğŸ§  ArXiv category dropdown
arxiv_categories = {
    "Machine Learning (cs.LG)": "cs.LG",
    "Computation and Language (cs.CL)": "cs.CL",
    "AI (cs.AI)": "cs.AI",
    "Computer Vision (cs.CV)": "cs.CV",
    "Other": "other"
}
selected_category = st.selectbox("Select ArXiv Category", options=list(arxiv_categories.keys()))

# ğŸ”§ Chunk count slider
k = st.slider("How many chunks to retrieve from ChromaDB?", min_value=3, max_value=15, value=6)

# ğŸ›ï¸ Select what to do with the retrieved chunks
task = st.radio(
    "Choose LLM task:",
    ["Extract Variables", "Summarize Paper"],
    horizontal=True
)


# ğŸ§  Query input
query = st.text_input("Enter your research query:")

if query:
    with st.spinner("ğŸ” Searching ChromaDB..."):
        chunks = search_chunks(query, k=k)

    if not chunks:
        st.warning("âš ï¸ No matching chunks found. Try a broader or more topic-specific query.")
        st.stop()

    st.markdown("### ğŸ“„ Retrieved Text Chunks:")

    for i, (chunk_text, meta) in enumerate(chunks):
        st.markdown(f"#### ğŸ§© Chunk {i+1} from **{meta.get('title', 'Unknown Title')}**")
        st.markdown(f"[ğŸ”— View paper PDF]({meta.get('url', '#')})")
        st.markdown(f"{chunk_text[:300]}...")

chunk_texts = [chunk for chunk, _ in chunks]

if task == "Extract Variables":
    with st.spinner("ğŸ§  Extracting variables..."):
        result = extract_variables_from_chunks(chunk_texts)
    st.markdown("### ğŸ§  Extracted Variables:")
    st.code(result)

elif task == "Summarize Paper":
    with st.spinner("ğŸ“ Generating summary..."):
        from rag_extract_variables import summarize_chunks
        result = summarize_chunks(chunk_texts)
    st.markdown("### âœ¨ Summary:")
    st.markdown(result)

