title,summary,pdf_url,authors,published,primary_category,variables
Concept Lancet: Image Editing with Compositional Representation Transplant,"Diffusion models are widely used for image editing tasks. Existing editing
methods often design a representation manipulation procedure by curating an
edit direction in the text embedding or score space. However, such a procedure
faces a key challenge: overestimating the edit strength harms visual
consistency while underestimating it fails the editing task. Notably, each
source image may require a different editing strength, and it is costly to
search for an appropriate strength via trial-and-error. To address this
challenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play
framework for principled representation manipulation in diffusion-based image
editing. At inference time, we decompose the source input in the latent (text
embedding or diffusion score) space as a sparse linear combination of the
representations of the collected visual concepts. This allows us to accurately
estimate the presence of concepts in each image, which informs the edit. Based
on the editing task (replace/add/remove), we perform a customized concept
transplant process to impose the corresponding editing direction. To
sufficiently model the concept space, we curate a conceptual representation
dataset, CoLan-150K, which contains diverse descriptions and scenarios of
visual terms and phrases for the latent dictionary. Experiments on multiple
diffusion-based image editing baselines show that methods equipped with CoLan
achieve state-of-the-art performance in editing effectiveness and consistency
preservation.",http://arxiv.org/pdf/2504.02828,"[arxiv.Result.Author('Jinqi Luo'), arxiv.Result.Author('Tianjiao Ding'), arxiv.Result.Author('Kwan Ho Ryan Chan'), arxiv.Result.Author('Hancheng Min'), arxiv.Result.Author('Chris Callison-Burch'), arxiv.Result.Author('René Vidal')]",2025-04-03 17:59:58+00:00,cs.CV,"Here are the independent and dependent variables identified in this study:

- **Independent Variable**: The image editing technique or method used (Concept Lancet or CoLan, which involves a zero-shot plug-and-play framework for representation manipulation in diffusion-based image editing).

- **Dependent Variables**: 
  - Editing effectiveness: This evaluates the success of the imposed concept edits based on user prompts (e.g., changing a cat to a dog).
  - Consistency preservation: This measures how well the visual consistency of the original image is maintained after the edits are applied. 

These variables are derived from the abstract and introduction which emphasize the enhancement of image editing through conceptual guidance and diffusion models."
Generative Evaluation of Complex Reasoning in Large Language Models,"With powerful large language models (LLMs) demonstrating superhuman reasoning
capabilities, a critical question arises: Do LLMs genuinely reason, or do they
merely recall answers from their extensive, web-scraped training datasets?
Publicly released benchmarks inevitably become contaminated once incorporated
into subsequent LLM training sets, undermining their reliability as faithful
assessments. To address this, we introduce KUMO, a generative evaluation
framework designed specifically for assessing reasoning in LLMs. KUMO
synergistically combines LLMs with symbolic engines to dynamically produce
diverse, multi-turn reasoning tasks that are partially observable and
adjustable in difficulty. Through an automated pipeline, KUMO continuously
generates novel tasks across open-ended domains, compelling models to
demonstrate genuine generalization rather than memorization. We evaluated 23
state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,
benchmarking their reasoning abilities against university students. Our
findings reveal that many LLMs have outperformed university-level performance
on easy reasoning tasks, and reasoning-scaled LLMs reach university-level
performance on complex reasoning challenges. Moreover, LLM performance on KUMO
tasks correlates strongly with results on newly released real-world reasoning
benchmarks, underscoring KUMO's value as a robust, enduring assessment tool for
genuine LLM reasoning capabilities.",http://arxiv.org/pdf/2504.02810,"[arxiv.Result.Author('Haowei Lin'), arxiv.Result.Author('Xiangyu Wang'), arxiv.Result.Author('Ruilin Yan'), arxiv.Result.Author('Baizhou Huang'), arxiv.Result.Author('Haotian Ye'), arxiv.Result.Author('Jianhua Zhu'), arxiv.Result.Author('Zihao Wang'), arxiv.Result.Author('James Zou'), arxiv.Result.Author('Jianzhu Ma'), arxiv.Result.Author('Yitao Liang')]",2025-04-03 17:54:18+00:00,cs.CL,"For the paper titled ""Generative evaluation of complex reasoning in large language models,"" the research focuses on evaluating the reasoning capabilities of large language models (LLMs). Here's a breakdown of the dependent and independent variables:

- **Dependent Variable:**
  - Performance of large language models (LLMs) on reasoning tasks. This is typically measured in terms of accuracy, generalization ability, and comparison to university students’ performance.

- **Independent Variable:**
  - The complexity and type of reasoning tasks generated by the KUMO evaluation framework. These tasks are dynamic, multi-turn, and vary in difficulty across different domains.

In summary, the study evaluates how well LLMs perform (dependent variable) when subjected to a variety of reasoning tasks generated by KUMO (independent variable)."
MegaMath: Pushing the Limits of Open Math Corpora,"Mathematical reasoning is a cornerstone of human intelligence and a key
benchmark for advanced capabilities in large language models (LLMs). However,
the research community still lacks an open, large-scale, high-quality corpus
tailored to the demands of math-centric LLM pre-training. We present MegaMath,
an open dataset curated from diverse, math-focused sources through following
practices: (1) Revisiting web data: We re-extracted mathematical documents from
Common Crawl with math-oriented HTML optimizations, fasttext-based filtering
and deduplication, all for acquiring higher-quality data on the Internet. (2)
Recalling Math-related code data: We identified high quality math-related code
from large code training corpus, Stack-V2, further enhancing data diversity.
(3) Exploring Synthetic data: We synthesized QA-style text, math-related code,
and interleaved text-code blocks from web data or code data. By integrating
these strategies and validating their effectiveness through extensive
ablations, MegaMath delivers 371B tokens with the largest quantity and top
quality among existing open math pre-training datasets.",http://arxiv.org/pdf/2504.02807,"[arxiv.Result.Author('Fan Zhou'), arxiv.Result.Author('Zengzhi Wang'), arxiv.Result.Author('Nikhil Ranjan'), arxiv.Result.Author('Zhoujun Cheng'), arxiv.Result.Author('Liping Tang'), arxiv.Result.Author('Guowei He'), arxiv.Result.Author('Zhengzhong Liu'), arxiv.Result.Author('Eric P. Xing')]",2025-04-03 17:52:07+00:00,cs.CL,"The dependent and independent variables in the study ""MegaMath: Pushing the Limits of Open Math Corpora"" are not explicitly outlined in a traditional experimental setup as this paper describes the creation and features of a dataset rather than conducting an experiment with clear variables. However, based on the content of the paper, we can infer the following:

- **Independent Variables:**
  - Sources of data (web data, math-related code data, synthetic data).
  - Data curation practices (re-extraction of documents, filtering, deduplication).

- **Dependent Variables:**
  - Quality and quantity of the math pre-training dataset.
  - Effectiveness of the dataset in mathematical reasoning tasks for large language models.

This study focuses on developing a large-scale, high-quality corpus tailored for mathematical pre-training, rather than testing specific hypotheses using control and test groups with dependent and independent variables as found in experimental research."
A Survey of Large Language Models in Mental Health Disorder Detection on Social Media,"The detection and intervention of mental health issues represent a critical
global research focus, and social media data has been recognized as an
important resource for mental health research. However, how to utilize Large
Language Models (LLMs) for mental health problem detection on social media
poses significant challenges. Hence, this paper aims to explore the potential
of LLM applications in social media data analysis, focusing not only on the
most common psychological disorders such as depression and anxiety but also
incorporating psychotic disorders and externalizing disorders, summarizing the
application methods of LLM from different dimensions, such as text data
analysis and detection of mental disorders, and revealing the major challenges
and shortcomings of current research. In addition, the paper provides an
overview of popular datasets, and evaluation metrics. The survey in this paper
provides a comprehensive frame of reference for researchers in the field of
mental health, while demonstrating the great potential of LLMs in mental health
detection to facilitate the further application of LLMs in future mental health
interventions.",http://arxiv.org/pdf/2504.02800,"[arxiv.Result.Author('Zhuohan Ge'), arxiv.Result.Author('Nicole Hu'), arxiv.Result.Author('Darian Li'), arxiv.Result.Author('Yubo Wang'), arxiv.Result.Author('Shihao Qi'), arxiv.Result.Author('Yuming Xu'), arxiv.Result.Author('Han Shi'), arxiv.Result.Author('Jason Zhang')]",2025-04-03 17:43:14+00:00,cs.CL,"In the study ""A Survey of Large Language Models in Mental Health Disorder Detection on Social Media,"" the dependent and independent variables can be outlined as follows:

### Independent Variables
- Use of Large Language Models (LLMs)
- Types of mental health disorders being analyzed (e.g., depression, anxiety, psychotic disorders, externalizing disorders)
- Social media data employed for analysis (user-generated content, interactions such as retweets, comments, likes)
- Different dimensions of LLM application methods, such as text data analysis and detection methods

### Dependent Variables
- Detection of mental health disorders on social media
- The effectiveness of LLMs in identifying and predicting mental health issues
- Accuracy and performance metrics related to mental health disorder detection using LLMs

The study primarily explores how LLMs can be utilized in detecting mental health disorders through social media data analysis and their potential impact on future mental health interventions."
"A Framework for Situating Innovations, Opportunities, and Challenges in Advancing Vertical Systems with Large AI Models","Large artificial intelligence (AI) models have garnered significant attention
for their remarkable, often ""superhuman"", performance on standardized
benchmarks. However, when these models are deployed in high-stakes verticals
such as healthcare, education, and law, they often reveal notable limitations.
For instance, they exhibit brittleness to minor variations in input data,
present contextually uninformed decisions in critical settings, and undermine
user trust by confidently producing or reproducing inaccuracies. These
challenges in applying large models necessitate cross-disciplinary innovations
to align the models' capabilities with the needs of real-world applications. We
introduce a framework that addresses this gap through a layer-wise abstraction
of innovations aimed at meeting users' requirements with large models. Through
multiple case studies, we illustrate how researchers and practitioners across
various fields can operationalize this framework. Beyond modularizing the
pipeline of transforming large models into useful ""vertical systems"", we also
highlight the dynamism that exists within different layers of the framework.
Finally, we discuss how our framework can guide researchers and practitioners
to (i) optimally situate their innovations (e.g., when vertical-specific
insights can empower broadly impactful vertical-agnostic innovations), (ii)
uncover overlooked opportunities (e.g., spotting recurring problems across
verticals to develop practically useful foundation models instead of chasing
benchmarks), and (iii) facilitate cross-disciplinary communication of critical
challenges (e.g., enabling a shared vocabulary for AI developers, domain
experts, and human-computer interaction scholars).",http://arxiv.org/pdf/2504.02793,"[arxiv.Result.Author('Gaurav Verma'), arxiv.Result.Author('Jiawei Zhou'), arxiv.Result.Author('Mohit Chandra'), arxiv.Result.Author('Srijan Kumar'), arxiv.Result.Author('Munmun De Choudhury')]",2025-04-03 17:40:11+00:00,cs.AI,"The extracted text doesn't directly specify an empirical study with traditional dependent and independent variables. Instead, it discusses a framework for addressing challenges with large AI models in vertical industries, citing examples, successes, and challenges these models face in real-world applications.

To identify the independent and dependent variables, we'd typically look for a specific experiment or observational study where certain elements are manipulated (independent variables) to observe the effect on other elements (dependent variables). Since this text outlines a conceptual framework rather than a specific experimental study, it doesn't provide these variables in the traditional research sense.

If the paper included an empirical study, the independent variables might involve specific configurations or adaptations of AI models, while the dependent variables could involve performance metrics such as accuracy, robustness, or user trust in AI systems. For precise identification, reviewing any experimental sections for variable descriptions would be necessary."
A Framework for Robust Cognitive Evaluation of LLMs,"Emergent cognitive abilities in large language models (LLMs) have been widely
observed, but their nature and underlying mechanisms remain poorly understood.
A growing body of research draws on cognitive science to investigate LLM
cognition, but standard methodologies and experimen-tal pipelines have not yet
been established. To address this gap we develop CognitivEval, a framework for
systematically evaluating the artificial cognitive capabilities of LLMs, with a
particular emphasis on robustness in response collection. The key features of
CognitivEval include: (i) automatic prompt permutations, and (ii) testing that
gathers both generations and model probability estimates. Our experiments
demonstrate that these features lead to more robust experimental outcomes.
Using CognitivEval, we replicate five classic experiments in cognitive science,
illustrating the framework's generalizability across various experimental tasks
and obtaining a cognitive profile of several state of the art LLMs.
CognitivEval will be released publicly to foster broader collaboration within
the cognitive science community.",http://arxiv.org/pdf/2504.02789,"[arxiv.Result.Author('Karin de Langis'), arxiv.Result.Author('Jong Inn Park'), arxiv.Result.Author('Bin Hu'), arxiv.Result.Author('Khanh Chi Le'), arxiv.Result.Author('Andreas Schramm'), arxiv.Result.Author('Michael C. Mensink'), arxiv.Result.Author('Andrew Elfenbein'), arxiv.Result.Author('Dongyeop Kang')]",2025-04-03 17:35:54+00:00,cs.CL,"Based on the content extracted from the research paper titled ""A Framework for Robust Cognitive Evaluation of LLMs,"" the study evaluates the artificial cognitive capabilities of large language models (LLMs) using a framework called COGNITIVEVAL. Here are the identified variables:

**Independent Variables:**
1. The method of prompt permutations: Variations in how prompts are formatted and worded.
2. The type of tasks or experiments being replicated from cognitive science (e.g., different classic experiments).
3. The specific large language models being evaluated.

**Dependent Variables:**
1. The responses generated by the LLMs to the given tasks or prompts.
2. The probability estimates provided by the LLMs for the target answers.

These variables are part of the study's focus on evaluating LLM cognition by replicating experiments and assessing outcomes with COGNITIVEVAL, a framework intended to enhance the robustness of experimental results in cognitive assessments of LLMs."
MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal Pairs,"We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic
minimal pairs, covering 101 languages, 6 linguistic phenomena and containing
more than 125,000 minimal pairs. Our minimal pairs are created using a fully
automated pipeline, leveraging the large-scale linguistic resources of
Universal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs
at an unprecedented multilingual scale, and highlights the shortcomings of the
current state-of-the-art in modelling low-resource languages.",http://arxiv.org/pdf/2504.02768,"[arxiv.Result.Author('Jaap Jumelet'), arxiv.Result.Author('Leonie Weissweiler'), arxiv.Result.Author('Arianna Bisazza')]",2025-04-03 17:05:50+00:00,cs.CL,"In the research paper ""MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal Pairs,"" the independent and dependent variables are as follows:

**Independent Variables:**
1. Linguistic phenomena: The study covers various linguistic phenomena such as subject-verb agreement for number, person, and gender.
2. Language: The benchmark evaluates linguistic abilities across 101 different languages.

**Dependent Variable:**
1. Large Language Model (LLM) performance: The study evaluates the abilities of LLMs in terms of accuracy and capability to assign higher probability to the grammatical version in syntactic minimal pair scenarios.

The study aims to understand how well LLMs perform across different languages and linguistic phenomena."
Enhancing LLM Robustness to Perturbed Instructions: An Empirical Study,"Large Language Models (LLMs) are highly vulnerable to input perturbations, as
even a small prompt change may result in a substantially different output.
Existing methods to enhance LLM robustness are primarily focused on perturbed
data samples, whereas improving resiliency to perturbations of task-level
instructions has remained relatively underexplored. In this work, we focus on
character- and word-level edits of task-specific instructions, which
substantially degrade downstream performance. We experiment with a variety of
techniques to enhance the robustness of LLMs, including self-denoising and
representation alignment, testing different models (Llama 3 and Flan-T5),
datasets (CoLA, QNLI, SST-2) and instructions (both task-oriented and
role-oriented). We find that, on average, self-denoising -- whether performed
by a frozen LLM or a fine-tuned model -- achieves substantially higher
performance gains than alternative strategies, including more complex baselines
such as ensembling and supervised methods.",http://arxiv.org/pdf/2504.02733,"[arxiv.Result.Author('Aryan Agrawal'), arxiv.Result.Author('Lisa Alazraki'), arxiv.Result.Author('Shahin Honarvar'), arxiv.Result.Author('Marek Rei')]",2025-04-03 16:17:56+00:00,cs.CL,"In the research paper titled ""ENHANCING LLM ROBUSTNESS TO PERTURBED INSTRUCTIONS: AN EMPIRICAL STUDY,"" the variables are as follows:

### Independent Variables:
1. **Types of Instruction Perturbations**: Word-level and character-level perturbations of task-specific instructions.
2. **Methods of Enhancing Robustness**: 
   - Self-denoising
   - Representation alignment
   - Other strategies such as ensembling and supervised methods
3. **Models Used**: Llama 3 and Flan-T5
4. **Types of Instructions**: Task-oriented and role-oriented
5. **Datasets Used**: CoLA, QNLI, SST-2

### Dependent Variable:
1. **LLM Performance**: The performance of the Large Language Models (LLMs) on classification tasks based on different methods and perturbations. This is measured by the degree to which the performance is affected by instruction perturbations.

The study focuses on how different variables, such as types of perturbations and methods for enhancing robustness, impact the performance of LLMs in conditions where task instructions are modified."
Why do LLMs attend to the first token?,"Large Language Models (LLMs) tend to attend heavily to the first token in the
sequence -- creating a so-called attention sink. Many works have studied this
phenomenon in detail, proposing various ways to either leverage or alleviate
it. Attention sinks have been connected to quantisation difficulties, security
issues, and streaming attention. Yet, while many works have provided conditions
in which they occur or not, a critical question remains shallowly answered: Why
do LLMs learn such patterns and how are they being used? In this work, we argue
theoretically and empirically that this mechanism provides a method for LLMs to
avoid over-mixing, connecting this to existing lines of work that study
mathematically how information propagates in Transformers. We conduct
experiments to validate our theoretical intuitions and show how choices such as
context length, depth, and data packing influence the sink behaviour. We hope
that this study provides a new practical perspective on why attention sinks are
useful in LLMs, leading to a better understanding of the attention patterns
that form during training.",http://arxiv.org/pdf/2504.02732,"[arxiv.Result.Author('Federico Barbero'), arxiv.Result.Author('Álvaro Arroyo'), arxiv.Result.Author('Xiangming Gu'), arxiv.Result.Author('Christos Perivolaropoulos'), arxiv.Result.Author('Michael Bronstein'), arxiv.Result.Author('Petar Veličkovi ć'), arxiv.Result.Author('Razvan Pascanu')]",2025-04-03 16:17:55+00:00,cs.CL,"From the information provided in the research paper, the study explores the phenomenon of ""attention sinks"" in Large Language Models (LLMs), specifically focusing on why LLMs tend to allocate significant attention to the first token in a sequence. The authors are interested in understanding the mechanisms behind this behavior and how various factors influence it. 

Here are the variables identified in this study:

**Dependent Variables:**
1. Attention Allocation: Specifically, the percentage or degree of attention focused on the first token (attention sink) in a sequence.
2. Effectiveness of Attention Sinking: This refers to the usefulness or utility of attention sinks in LLMs, as evaluated through empirical experiments.

**Independent Variables:**
1. Context Length: The length of the input sequence or context that the model processes.
2. Model Depth: The number of layers in the Transformer architecture of the LLMs being studied.
3. Data Packing: How data is structured or packed for processing in the model.
4. Model Training Configurations: Various configurations or settings under which the models are trained, including training from scratch.

The study conducts experiments to empirically validate theoretical intuitions about attention sinks, considering how modifications in the independent variables influence the dependent variables."
ERPO: Advancing Safety Alignment via Ex-Ante Reasoning Preference Optimization,"Recent advancements in large language models (LLMs) have accelerated progress
toward artificial general intelligence, yet their potential to generate harmful
content poses critical safety challenges. Existing alignment methods often
struggle to cover diverse safety scenarios and remain vulnerable to adversarial
attacks. In this work, we propose Ex-Ante Reasoning Preference Optimization
(ERPO), a novel safety alignment framework that equips LLMs with explicit
preemptive reasoning through Chain-of-Thought and provides clear evidence for
safety judgments by embedding predefined safety rules. Specifically, our
approach consists of three stages: first, equipping the model with Ex-Ante
reasoning through supervised fine-tuning (SFT) using a constructed reasoning
module; second, enhancing safety, usefulness, and efficiency via Direct
Preference Optimization (DPO); and third, mitigating inference latency with a
length-controlled iterative preference optimization strategy. Experiments on
multiple open-source LLMs demonstrate that ERPO significantly enhances safety
performance while maintaining response efficiency.",http://arxiv.org/pdf/2504.02725,"[arxiv.Result.Author('Kehua Feng'), arxiv.Result.Author('Keyan Ding'), arxiv.Result.Author('Jing Yu'), arxiv.Result.Author('Menghan Li'), arxiv.Result.Author('Yuhao Wang'), arxiv.Result.Author('Tong Xu'), arxiv.Result.Author('Xinda Wang'), arxiv.Result.Author('Qiang Zhang'), arxiv.Result.Author('Huajun Chen')]",2025-04-03 16:07:38+00:00,cs.CL,"The paper ""ERPO: Advancing Safety Alignment via Ex-Ante Reasoning Preference Optimization"" focuses on the effectiveness of a new framework for improving safety alignment in large language models. Here are the independent and dependent variables identified in the study:

- **Independent Variables:**
  - The application of the Ex-Ante Reasoning Preference Optimization (ERPO) framework, which includes:
    - Supervised Fine-Tuning (SFT) with a constructed reasoning module.
    - Direct Preference Optimization (DPO).
    - Length-controlled iterative preference optimization strategy.

- **Dependent Variables:**
  - Safety performance of the large language models.
  - Response efficiency of the models after applying the ERPO framework.

The study measures how the introduction and application of the ERPO framework, which is the independent variable, affect safety alignment and response efficiency, which are the dependent variables."
