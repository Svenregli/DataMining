title,summary,pdf_url,authors,published,primary_category,variables
Concept Lancet: Image Editing with Compositional Representation Transplant,"Diffusion models are widely used for image editing tasks. Existing editing
methods often design a representation manipulation procedure by curating an
edit direction in the text embedding or score space. However, such a procedure
faces a key challenge: overestimating the edit strength harms visual
consistency while underestimating it fails the editing task. Notably, each
source image may require a different editing strength, and it is costly to
search for an appropriate strength via trial-and-error. To address this
challenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play
framework for principled representation manipulation in diffusion-based image
editing. At inference time, we decompose the source input in the latent (text
embedding or diffusion score) space as a sparse linear combination of the
representations of the collected visual concepts. This allows us to accurately
estimate the presence of concepts in each image, which informs the edit. Based
on the editing task (replace/add/remove), we perform a customized concept
transplant process to impose the corresponding editing direction. To
sufficiently model the concept space, we curate a conceptual representation
dataset, CoLan-150K, which contains diverse descriptions and scenarios of
visual terms and phrases for the latent dictionary. Experiments on multiple
diffusion-based image editing baselines show that methods equipped with CoLan
achieve state-of-the-art performance in editing effectiveness and consistency
preservation.",http://arxiv.org/pdf/2504.02828,"[arxiv.Result.Author('Jinqi Luo'), arxiv.Result.Author('Tianjiao Ding'), arxiv.Result.Author('Kwan Ho Ryan Chan'), arxiv.Result.Author('Hancheng Min'), arxiv.Result.Author('Chris Callison-Burch'), arxiv.Result.Author('René Vidal')]",2025-04-03 17:59:58+00:00,cs.CV,"Independent Variables: source image, target prompt, editing task

Dependent Variables: content modification, appearance modification, pattern modification, editing effectiveness, consistency preservation"
Generative Evaluation of Complex Reasoning in Large Language Models,"With powerful large language models (LLMs) demonstrating superhuman reasoning
capabilities, a critical question arises: Do LLMs genuinely reason, or do they
merely recall answers from their extensive, web-scraped training datasets?
Publicly released benchmarks inevitably become contaminated once incorporated
into subsequent LLM training sets, undermining their reliability as faithful
assessments. To address this, we introduce KUMO, a generative evaluation
framework designed specifically for assessing reasoning in LLMs. KUMO
synergistically combines LLMs with symbolic engines to dynamically produce
diverse, multi-turn reasoning tasks that are partially observable and
adjustable in difficulty. Through an automated pipeline, KUMO continuously
generates novel tasks across open-ended domains, compelling models to
demonstrate genuine generalization rather than memorization. We evaluated 23
state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,
benchmarking their reasoning abilities against university students. Our
findings reveal that many LLMs have outperformed university-level performance
on easy reasoning tasks, and reasoning-scaled LLMs reach university-level
performance on complex reasoning challenges. Moreover, LLM performance on KUMO
tasks correlates strongly with results on newly released real-world reasoning
benchmarks, underscoring KUMO's value as a robust, enduring assessment tool for
genuine LLM reasoning capabilities.",http://arxiv.org/pdf/2504.02810,"[arxiv.Result.Author('Haowei Lin'), arxiv.Result.Author('Xiangyu Wang'), arxiv.Result.Author('Ruilin Yan'), arxiv.Result.Author('Baizhou Huang'), arxiv.Result.Author('Haotian Ye'), arxiv.Result.Author('Jianhua Zhu'), arxiv.Result.Author('Zihao Wang'), arxiv.Result.Author('James Zou'), arxiv.Result.Author('Jianzhu Ma'), arxiv.Result.Author('Yitao Liang')]",2025-04-03 17:54:18+00:00,cs.CL,"Independent Variables: Large Language Models (LLMs), KUMO framework, complexity of reasoning tasks

Dependent Variables: LLMs reasoning performance, comparison to university students, correlation with real-world benchmarks"
MegaMath: Pushing the Limits of Open Math Corpora,"Mathematical reasoning is a cornerstone of human intelligence and a key
benchmark for advanced capabilities in large language models (LLMs). However,
the research community still lacks an open, large-scale, high-quality corpus
tailored to the demands of math-centric LLM pre-training. We present MegaMath,
an open dataset curated from diverse, math-focused sources through following
practices: (1) Revisiting web data: We re-extracted mathematical documents from
Common Crawl with math-oriented HTML optimizations, fasttext-based filtering
and deduplication, all for acquiring higher-quality data on the Internet. (2)
Recalling Math-related code data: We identified high quality math-related code
from large code training corpus, Stack-V2, further enhancing data diversity.
(3) Exploring Synthetic data: We synthesized QA-style text, math-related code,
and interleaved text-code blocks from web data or code data. By integrating
these strategies and validating their effectiveness through extensive
ablations, MegaMath delivers 371B tokens with the largest quantity and top
quality among existing open math pre-training datasets.",http://arxiv.org/pdf/2504.02807,"[arxiv.Result.Author('Fan Zhou'), arxiv.Result.Author('Zengzhi Wang'), arxiv.Result.Author('Nikhil Ranjan'), arxiv.Result.Author('Zhoujun Cheng'), arxiv.Result.Author('Liping Tang'), arxiv.Result.Author('Guowei He'), arxiv.Result.Author('Zhengzhong Liu'), arxiv.Result.Author('Eric P. Xing')]",2025-04-03 17:52:07+00:00,cs.CL,"The document primarily describes the MegaMath dataset used for evaluating mathematical reasoning capabilities in language models rather than a study with specific dependent and independent variables. Therefore, the paper does not delineate traditional independent or dependent variables typically used in experimental studies. However, if variables were to be considered abstractly in terms of research focus, they might be:

Independent Variables: Dataset composition, data source (web data, math-related code, synthetic data), filtering strategy, deduplication strategy.

Dependent Variables: Data quality, number of training tokens, model performance on mathematical reasoning tasks."
A Survey of Large Language Models in Mental Health Disorder Detection on Social Media,"The detection and intervention of mental health issues represent a critical
global research focus, and social media data has been recognized as an
important resource for mental health research. However, how to utilize Large
Language Models (LLMs) for mental health problem detection on social media
poses significant challenges. Hence, this paper aims to explore the potential
of LLM applications in social media data analysis, focusing not only on the
most common psychological disorders such as depression and anxiety but also
incorporating psychotic disorders and externalizing disorders, summarizing the
application methods of LLM from different dimensions, such as text data
analysis and detection of mental disorders, and revealing the major challenges
and shortcomings of current research. In addition, the paper provides an
overview of popular datasets, and evaluation metrics. The survey in this paper
provides a comprehensive frame of reference for researchers in the field of
mental health, while demonstrating the great potential of LLMs in mental health
detection to facilitate the further application of LLMs in future mental health
interventions.",http://arxiv.org/pdf/2504.02800,"[arxiv.Result.Author('Zhuohan Ge'), arxiv.Result.Author('Nicole Hu'), arxiv.Result.Author('Darian Li'), arxiv.Result.Author('Yubo Wang'), arxiv.Result.Author('Shihao Qi'), arxiv.Result.Author('Yuming Xu'), arxiv.Result.Author('Han Shi'), arxiv.Result.Author('Jason Zhang')]",2025-04-03 17:43:14+00:00,cs.CL,"Independent Variables: Use of Large Language Models (LLMs), Social media data

Dependent Variables: Detection of mental health disorders, Accuracy of disorder detection, Effectiveness in analyzing social media content"
"A Framework for Situating Innovations, Opportunities, and Challenges in Advancing Vertical Systems with Large AI Models","Large artificial intelligence (AI) models have garnered significant attention
for their remarkable, often ""superhuman"", performance on standardized
benchmarks. However, when these models are deployed in high-stakes verticals
such as healthcare, education, and law, they often reveal notable limitations.
For instance, they exhibit brittleness to minor variations in input data,
present contextually uninformed decisions in critical settings, and undermine
user trust by confidently producing or reproducing inaccuracies. These
challenges in applying large models necessitate cross-disciplinary innovations
to align the models' capabilities with the needs of real-world applications. We
introduce a framework that addresses this gap through a layer-wise abstraction
of innovations aimed at meeting users' requirements with large models. Through
multiple case studies, we illustrate how researchers and practitioners across
various fields can operationalize this framework. Beyond modularizing the
pipeline of transforming large models into useful ""vertical systems"", we also
highlight the dynamism that exists within different layers of the framework.
Finally, we discuss how our framework can guide researchers and practitioners
to (i) optimally situate their innovations (e.g., when vertical-specific
insights can empower broadly impactful vertical-agnostic innovations), (ii)
uncover overlooked opportunities (e.g., spotting recurring problems across
verticals to develop practically useful foundation models instead of chasing
benchmarks), and (iii) facilitate cross-disciplinary communication of critical
challenges (e.g., enabling a shared vocabulary for AI developers, domain
experts, and human-computer interaction scholars).",http://arxiv.org/pdf/2504.02793,"[arxiv.Result.Author('Gaurav Verma'), arxiv.Result.Author('Jiawei Zhou'), arxiv.Result.Author('Mohit Chandra'), arxiv.Result.Author('Srijan Kumar'), arxiv.Result.Author('Munmun De Choudhury')]",2025-04-03 17:40:11+00:00,cs.AI,"The paper does not appear to be a traditional empirical research study with clear independent and dependent variables, but rather a conceptual framework discussing innovations and challenges in AI model deployment across vertical systems. Therefore, there are no defined independent and dependent variables in the typical sense. The focus is on framework and application rather than specific experimental variables."
A Framework for Robust Cognitive Evaluation of LLMs,"Emergent cognitive abilities in large language models (LLMs) have been widely
observed, but their nature and underlying mechanisms remain poorly understood.
A growing body of research draws on cognitive science to investigate LLM
cognition, but standard methodologies and experimen-tal pipelines have not yet
been established. To address this gap we develop CognitivEval, a framework for
systematically evaluating the artificial cognitive capabilities of LLMs, with a
particular emphasis on robustness in response collection. The key features of
CognitivEval include: (i) automatic prompt permutations, and (ii) testing that
gathers both generations and model probability estimates. Our experiments
demonstrate that these features lead to more robust experimental outcomes.
Using CognitivEval, we replicate five classic experiments in cognitive science,
illustrating the framework's generalizability across various experimental tasks
and obtaining a cognitive profile of several state of the art LLMs.
CognitivEval will be released publicly to foster broader collaboration within
the cognitive science community.",http://arxiv.org/pdf/2504.02789,"[arxiv.Result.Author('Karin de Langis'), arxiv.Result.Author('Jong Inn Park'), arxiv.Result.Author('Bin Hu'), arxiv.Result.Author('Khanh Chi Le'), arxiv.Result.Author('Andreas Schramm'), arxiv.Result.Author('Michael C. Mensink'), arxiv.Result.Author('Andrew Elfenbein'), arxiv.Result.Author('Dongyeop Kang')]",2025-04-03 17:35:54+00:00,cs.CL,"It seems that the extracted text does not include clear details on specific variables being studied, likely due to the nature of the text extraction from the document. However, based on the summary provided, I can infer some potential variables involved.

### Independent Variables:
- Prompt formatting and wording (automatic prompt permutations)
- Experimental tasks replicated from cognitive science

### Dependent Variables:
- LLM responses
- LLM’s internal probability estimates of the target answer

These are inferred categories based on the information about framework features and testing outcomes noted in the abstract and introductory sections of the paper. For precise variables, one would typically look into the detailed methodology and results sections of the paper."
MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal Pairs,"We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic
minimal pairs, covering 101 languages, 6 linguistic phenomena and containing
more than 125,000 minimal pairs. Our minimal pairs are created using a fully
automated pipeline, leveraging the large-scale linguistic resources of
Universal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs
at an unprecedented multilingual scale, and highlights the shortcomings of the
current state-of-the-art in modelling low-resource languages.",http://arxiv.org/pdf/2504.02768,"[arxiv.Result.Author('Jaap Jumelet'), arxiv.Result.Author('Leonie Weissweiler'), arxiv.Result.Author('Arianna Bisazza')]",2025-04-03 17:05:50+00:00,cs.CL,"Independent Variables: Language, Linguistic Phenomenon (subject-verb agreement, subject-participle agreement)

Dependent Variables: Model's Accuracy, Probability assigned to grammatical versus ungrammatical sentence versions"
Enhancing LLM Robustness to Perturbed Instructions: An Empirical Study,"Large Language Models (LLMs) are highly vulnerable to input perturbations, as
even a small prompt change may result in a substantially different output.
Existing methods to enhance LLM robustness are primarily focused on perturbed
data samples, whereas improving resiliency to perturbations of task-level
instructions has remained relatively underexplored. In this work, we focus on
character- and word-level edits of task-specific instructions, which
substantially degrade downstream performance. We experiment with a variety of
techniques to enhance the robustness of LLMs, including self-denoising and
representation alignment, testing different models (Llama 3 and Flan-T5),
datasets (CoLA, QNLI, SST-2) and instructions (both task-oriented and
role-oriented). We find that, on average, self-denoising -- whether performed
by a frozen LLM or a fine-tuned model -- achieves substantially higher
performance gains than alternative strategies, including more complex baselines
such as ensembling and supervised methods.",http://arxiv.org/pdf/2504.02733,"[arxiv.Result.Author('Aryan Agrawal'), arxiv.Result.Author('Lisa Alazraki'), arxiv.Result.Author('Shahin Honarvar'), arxiv.Result.Author('Marek Rei')]",2025-04-03 16:17:56+00:00,cs.CL,"Independent Variables: character-level edits, word-level edits, instruction types, perturbation types, models (Llama 3, Flan-T5), datasets (CoLA, QNLI, SST-2), self-denoising techniques, representation alignment

Dependent Variables: LLM robustness, performance gains, downstream performance, instruction comprehension"
Why do LLMs attend to the first token?,"Large Language Models (LLMs) tend to attend heavily to the first token in the
sequence -- creating a so-called attention sink. Many works have studied this
phenomenon in detail, proposing various ways to either leverage or alleviate
it. Attention sinks have been connected to quantisation difficulties, security
issues, and streaming attention. Yet, while many works have provided conditions
in which they occur or not, a critical question remains shallowly answered: Why
do LLMs learn such patterns and how are they being used? In this work, we argue
theoretically and empirically that this mechanism provides a method for LLMs to
avoid over-mixing, connecting this to existing lines of work that study
mathematically how information propagates in Transformers. We conduct
experiments to validate our theoretical intuitions and show how choices such as
context length, depth, and data packing influence the sink behaviour. We hope
that this study provides a new practical perspective on why attention sinks are
useful in LLMs, leading to a better understanding of the attention patterns
that form during training.",http://arxiv.org/pdf/2504.02732,"[arxiv.Result.Author('Federico Barbero'), arxiv.Result.Author('Álvaro Arroyo'), arxiv.Result.Author('Xiangming Gu'), arxiv.Result.Author('Christos Perivolaropoulos'), arxiv.Result.Author('Michael Bronstein'), arxiv.Result.Author('Petar Veličkovi ć'), arxiv.Result.Author('Razvan Pascanu')]",2025-04-03 16:17:55+00:00,cs.CL,"Independent Variables: context length, model depth, data packing choices

Dependent Variables: attention sink behavior, attention pattern formation"
ERPO: Advancing Safety Alignment via Ex-Ante Reasoning Preference Optimization,"Recent advancements in large language models (LLMs) have accelerated progress
toward artificial general intelligence, yet their potential to generate harmful
content poses critical safety challenges. Existing alignment methods often
struggle to cover diverse safety scenarios and remain vulnerable to adversarial
attacks. In this work, we propose Ex-Ante Reasoning Preference Optimization
(ERPO), a novel safety alignment framework that equips LLMs with explicit
preemptive reasoning through Chain-of-Thought and provides clear evidence for
safety judgments by embedding predefined safety rules. Specifically, our
approach consists of three stages: first, equipping the model with Ex-Ante
reasoning through supervised fine-tuning (SFT) using a constructed reasoning
module; second, enhancing safety, usefulness, and efficiency via Direct
Preference Optimization (DPO); and third, mitigating inference latency with a
length-controlled iterative preference optimization strategy. Experiments on
multiple open-source LLMs demonstrate that ERPO significantly enhances safety
performance while maintaining response efficiency.",http://arxiv.org/pdf/2504.02725,"[arxiv.Result.Author('Kehua Feng'), arxiv.Result.Author('Keyan Ding'), arxiv.Result.Author('Jing Yu'), arxiv.Result.Author('Menghan Li'), arxiv.Result.Author('Yuhao Wang'), arxiv.Result.Author('Tong Xu'), arxiv.Result.Author('Xinda Wang'), arxiv.Result.Author('Qiang Zhang'), arxiv.Result.Author('Huajun Chen')]",2025-04-03 16:07:38+00:00,cs.CL,"Independent Variables: Ex-Ante reasoning, supervised fine-tuning, Direct Preference Optimization, length-controlled iterative preference optimization

Dependent Variables: safety performance, response efficiency"
