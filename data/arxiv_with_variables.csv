title,summary,pdf_url,authors,published,primary_category,variables,independent,dependent
Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning,"Continual learning in large language models (LLMs) is prone to catastrophic
forgetting, where adapting to new tasks significantly degrades performance on
previously learned ones. Existing methods typically rely on low-rank,
parameter-efficient updates that limit the model's expressivity and introduce
additional parameters per task, leading to scalability issues. To address these
limitations, we propose a novel continual full fine-tuning approach leveraging
adaptive singular value decomposition (SVD). Our method dynamically identifies
task-specific low-rank parameter subspaces and constrains updates to be
orthogonal to critical directions associated with prior tasks, thus effectively
minimizing interference without additional parameter overhead or storing
previous task gradients. We evaluate our approach extensively on standard
continual learning benchmarks using both encoder-decoder (T5-Large) and
decoder-only (LLaMA-2 7B) models, spanning diverse tasks including
classification, generation, and reasoning. Empirically, our method achieves
state-of-the-art results, up to 7% higher average accuracy than recent
baselines like O-LoRA, and notably maintains the model's general linguistic
capabilities, instruction-following accuracy, and safety throughout the
continual learning process by reducing forgetting to near-negligible levels.
Our adaptive SVD framework effectively balances model plasticity and knowledge
retention, providing a practical, theoretically grounded, and computationally
scalable solution for continual learning scenarios in large language models.",http://arxiv.org/pdf/2504.07097,"['Nikhil Shivakumar Nayak', 'Krishnateja Killamsetty', 'Ligong Han', 'Abhishek Bhandwaldar', 'Prateek Chanda', 'Kai Xu', 'Hao Wang', 'Aldo Pareja', 'Oleg Silkin', 'Mustafa Eyceoz', 'Akash Srivastava']",2025-04-09 17:59:42+00:00,cs.LG,"Independent Variables: task-specific low-rank parameter subspaces, orthogonal update constraints, continual learning benchmarks, model type (encoder-decoder T5-Large, decoder-only LLaMA-2 7B)

Dependent Variables: model expressivity, performance degradation, average accuracy, knowledge retention, forgetting reduction, computational scalability","task-specific low-rank parameter subspaces, orthogonal update constraints, continual learning benchmarks, model type (encoder-decoder T5-Large, decoder-only LLaMA-2 7B)","model expressivity, performance degradation, average accuracy, knowledge retention, forgetting reduction, computational scalability"
OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens,"We present OLMoTrace, the first system that traces the outputs of language
models back to their full, multi-trillion-token training data in real time.
OLMoTrace finds and shows verbatim matches between segments of language model
output and documents in the training text corpora. Powered by an extended
version of infini-gram (Liu et al., 2024), our system returns tracing results
within a few seconds. OLMoTrace can help users understand the behavior of
language models through the lens of their training data. We showcase how it can
be used to explore fact checking, hallucination, and the creativity of language
models. OLMoTrace is publicly available and fully open-source.",http://arxiv.org/pdf/2504.07096,"['Jiacheng Liu', 'Taylor Blanton', 'Yanai Elazar', 'Sewon Min', 'YenSung Chen', 'Arnavi Chheda-Kothary', 'Huy Tran', 'Byron Bischoff', 'Eric Marsh', 'Michael Schmitz', 'Cassidy Trier', 'Aaron Sarnat', 'Jenna James', 'Jon Borchardt', 'Bailey Kuehl', 'Evie Cheng', 'Karen Farley', 'Sruthi Sreeram', 'Taira Anderson', 'David Albright', 'Carissa Schoenick', 'Luca Soldaini', 'Dirk Groeneveld', 'Rock Yuren Pang', 'Pang Wei Koh', 'Noah A. Smith', 'Sophie Lebrecht', 'Yejin Choi', 'Hannaneh Hajishirzi', 'Ali Farhadi', 'Jesse Dodge']",2025-04-09 17:59:35+00:00,cs.CL,"Independent Variables: Language model outputs, Training data corpus.
Dependent Variables: Tracing accuracy, Response generation relevance, Real-time tracing speed.","Language model outputs, Training data corpus.","Tracing accuracy, Response generation relevance, Real-time tracing speed."
OmniCaptioner: One Captioner to Rule Them All,"We propose OmniCaptioner, a versatile visual captioning framework for
generating fine-grained textual descriptions across a wide variety of visual
domains. Unlike prior methods limited to specific image types (e.g., natural
images or geometric visuals), our framework provides a unified solution for
captioning natural images, visual text (e.g., posters, UIs, textbooks), and
structured visuals (e.g., documents, tables, charts). By converting low-level
pixel information into semantically rich textual representations, our framework
bridges the gap between visual and textual modalities. Our results highlight
three key advantages: (i) Enhanced Visual Reasoning with LLMs, where
long-context captions of visual modalities empower LLMs, particularly the
DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)
Improved Image Generation, where detailed captions improve tasks like
text-to-image generation and image transformation; and (iii) Efficient
Supervised Fine-Tuning (SFT), which enables faster convergence with less data.
We believe the versatility and adaptability of OmniCaptioner can offer a new
perspective for bridging the gap between language and visual modalities.",http://arxiv.org/pdf/2504.07089,"['Yiting Lu', 'Jiakang Yuan', 'Zhen Li', 'Shitian Zhao', 'Qi Qin', 'Xinyue Li', 'Le Zhuo', 'Licheng Wen', 'Dongyang Liu', 'Yuewen Cao', 'Xiangchao Yan', 'Xin Li', 'Botian Shi', 'Tao Chen', 'Zhibo Chen', 'Lei Bai', 'Bo Zhang', 'Peng Gao']",2025-04-09 17:58:58+00:00,cs.CV,"It appears that the extracted text does not contain enough information to clearly identify the independent and dependent variables from the study. To proceed, I will need to analyze the entire paper to identify these variables. However, based on the introduction and abstract provided, I can attempt to infer:

- **Independent Variables**: Visual domains (natural images, visual text, structured visuals), captioning frameworks, pretraining methods.
  
- **Dependent Variables**: Visual reasoning capability, image generation quality, efficiency of supervised fine-tuning.

If you require a precise identification, it is essential to examine the full text for specific sections discussing methods or experiments that detail these variables. Let me know if you would like me to continue with a further extraction or provide more information.",,
KG-LLM-Bench: A Scalable Benchmark for Evaluating LLM Reasoning on Textualized Knowledge Graphs,"Knowledge graphs have emerged as a popular method for injecting up-to-date,
factual knowledge into large language models (LLMs). This is typically achieved
by converting the knowledge graph into text that the LLM can process in
context. While multiple methods of encoding knowledge graphs have been
proposed, the impact of this textualization process on LLM performance remains
under-explored. We introduce KG-LLM-Bench, a comprehensive and extensible
benchmark spanning five knowledge graph understanding tasks, and evaluate how
different encoding strategies affect performance across various base models.
Our extensive experiments with seven language models and five textualization
strategies provide insights for optimizing LLM performance on KG reasoning
tasks.",http://arxiv.org/pdf/2504.07087,"['Elan Markowitz', 'Krupa Galiya', 'Greg Ver Steeg', 'Aram Galstyan']",2025-04-09 17:58:47+00:00,cs.CL,"Independent Variables: textualization strategies, language models

Dependent Variables: LLM performance on KG reasoning tasks, benchmark performance differences","textualization strategies, language models","LLM performance on KG reasoning tasks, benchmark performance differences"
A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility,"Reasoning has emerged as the next major frontier for language models (LMs),
with rapid advances from both academic and industrial labs. However, this
progress often outpaces methodological rigor, with many evaluations relying on
benchmarking practices that lack transparency, robustness, or statistical
grounding. In this work, we conduct a comprehensive empirical study and find
that current mathematical reasoning benchmarks are highly sensitive to subtle
implementation choices - including decoding parameters, random seeds, prompt
formatting, and even hardware and software-framework configurations.
Performance gains reported in recent studies frequently hinge on unclear
comparisons or unreported sources of variance. To address these issues, we
propose a standardized evaluation framework with clearly defined best practices
and reporting standards. Using this framework, we reassess recent methods and
find that reinforcement learning (RL) approaches yield only modest improvements
- far below prior claims - and are prone to overfitting, especially on
small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT)
methods show consistently stronger generalization. To foster reproducibility,
we release all code, prompts, and model outputs, for reasoning benchmarks,
establishing more rigorous foundations for future work.",http://arxiv.org/pdf/2504.07086,"['Andreas Hochlehnert', 'Hardik Bhatnagar', 'Vishaal Udandarao', 'Samuel Albanie', 'Ameya Prabhu', 'Matthias Bethge']",2025-04-09 17:58:17+00:00,cs.LG,"Independent Variables: decoding parameters, random seeds, prompt formatting, hardware configurations, software-framework configurations

Dependent Variables: performance on mathematical reasoning benchmarks, generalization strength","decoding parameters, random seeds, prompt formatting, hardware configurations, software-framework configurations","performance on mathematical reasoning benchmarks, generalization strength"
Self-Steering Language Models,"While test-time reasoning enables language models to tackle complex tasks,
searching or planning in natural language can be slow, costly, and error-prone.
But even when LMs struggle to emulate the precise reasoning steps needed to
solve a problem, they often excel at describing its abstract structure--both
how to verify solutions and how to search for them. This paper introduces
DisCIPL, a method for ""self-steering"" LMs where a Planner model generates a
task-specific inference program that is executed by a population of Follower
models. Our approach equips LMs with the ability to write recursive search
procedures that guide LM inference, enabling new forms of verifiable and
efficient reasoning. When instantiated with a small Follower (e.g.,
Llama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models,
including GPT-4o and o1, on challenging constrained generation tasks. In
decoupling planning from execution, our work opens up a design space of
highly-parallelized Monte Carlo inference strategies that outperform standard
best-of-N sampling, require no finetuning, and can be implemented automatically
by existing LMs.",http://arxiv.org/pdf/2504.07081,"['Gabriel Grand', 'Joshua B. Tenenbaum', 'Vikash K. Mansinghka', 'Alexander K. Lew', 'Jacob Andreas']",2025-04-09 17:54:22+00:00,cs.CL,"Independent Variables: DISCIPL method, Planner model setting and usage, Follower model characteristics, task-specific inference program

Dependent Variables: Model performance on constrained generation tasks, comparison with larger models, efficiency of reasoning, success in addressing constrained generation tasks","DISCIPL method, Planner model setting and usage, Follower model characteristics, task-specific inference program","Model performance on constrained generation tasks, comparison with larger models, efficiency of reasoning, success in addressing constrained generation tasks"
DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning,"Despite great performance on Olympiad-level reasoning problems, frontier
large language models can still struggle on high school math when presented
with novel problems outside standard benchmarks. Going beyond final accuracy,
we propose a deductive consistency metric to analyze chain-of-thought output
from language models (LMs).Formally, deductive reasoning involves two subtasks:
understanding a set of input premises and inferring the conclusions that follow
from them. The proposed metric studies LMs' performance on these subtasks, with
the goal of explaining LMs' reasoning errors on novel problems: how well do LMs
understand input premises with increasing context lengths, and how well can
they infer conclusions over multiple reasoning hops? Since existing benchmarks
may be memorized, we develop a pipeline to evaluate LMs' deductive consistency
on novel, perturbed versions of benchmark problems. On novel grade school math
problems (GSM-8k), we find that LMs are fairly robust to increasing number of
input premises, but suffer significant accuracy decay as the number of
reasoning hops is increased. Interestingly, these errors are masked in the
original benchmark as all models achieve near 100% accuracy. As we increase the
number of solution steps using a synthetic dataset, prediction over multiple
hops still remains the major source of error compared to understanding input
premises. Other factors, such as shifts in language style or natural
propagation of early errors do not explain the trends. Our analysis provides a
new view to characterize LM reasoning -- as computations over a window of input
premises and reasoning hops -- that can provide unified evaluation across
problem domains.",http://arxiv.org/pdf/2504.07080,"['Atharva Pandey', 'Kshitij Dubey', 'Rahul Sharma', 'Amit Sharma']",2025-04-09 17:53:55+00:00,cs.CL,"The provided excerpt from the paper describes a study focusing on evaluating language models' reasoning capabilities using deductive consistency as a metric. Based on this, the independent and dependent variables can be identified as follows:

Independent Variables: Number of input premises, number of reasoning hops, type of reasoning problem (e.g., novel vs. benchmark problems), shifts in language style

Dependent Variables: Deductive consistency, reasoning accuracy, reasoning errors, robustness to input premises, performance on novel problems","Number of input premises, number of reasoning hops, type of reasoning problem (e.g., novel vs. benchmark problems), shifts in language style","Deductive consistency, reasoning accuracy, reasoning errors, robustness to input premises, performance on novel problems"
SkillWeaver: Web Agents can Self-Improve by Discovering and Honing Skills,"To survive and thrive in complex environments, humans have evolved
sophisticated self-improvement mechanisms through environment exploration,
hierarchical abstraction of experiences into reuseable skills, and
collaborative construction of an ever-growing skill repertoire. Despite recent
advancements, autonomous web agents still lack crucial self-improvement
capabilities, struggling with procedural knowledge abstraction, refining
skills, and skill composition. In this work, we introduce SkillWeaver, a
skill-centric framework enabling agents to self-improve by autonomously
synthesizing reusable skills as APIs. Given a new website, the agent
autonomously discovers skills, executes them for practice, and distills
practice experiences into robust APIs. Iterative exploration continually
expands a library of lightweight, plug-and-play APIs, significantly enhancing
the agent's capabilities. Experiments on WebArena and real-world websites
demonstrate the efficacy of SkillWeaver, achieving relative success rate
improvements of 31.8% and 39.8%, respectively. Additionally, APIs synthesized
by strong agents substantially enhance weaker agents through transferable
skills, yielding improvements of up to 54.3% on WebArena. These results
demonstrate the effectiveness of honing diverse website interactions into APIs,
which can be seamlessly shared among various web agents.",http://arxiv.org/pdf/2504.07079,"['Boyuan Zheng', 'Michael Y. Fatemi', 'Xiaolong Jin', 'Zora Zhiruo Wang', 'Apurva Gandhi', 'Yueqi Song', 'Yu Gu', 'Jayanth Srinivasa', 'Gaowen Liu', 'Graham Neubig', 'Yu Su']",2025-04-09 17:51:50+00:00,cs.AI,"Independent Variables: Web agents, website environments, skill frameworks, exploration methods

Dependent Variables: Agent capabilities, success rate improvements, API robustness, skill transferability","Web agents, website environments, skill frameworks, exploration methods","Agent capabilities, success rate improvements, API robustness, skill transferability"
Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation,"The evaluation of vision-language models (VLMs) has mainly relied on
English-language benchmarks, leaving significant gaps in both multilingual and
multicultural coverage. While multilingual benchmarks have expanded, both in
size and languages, many rely on translations of English datasets, failing to
capture cultural nuances. In this work, we propose Kaleidoscope, as the most
comprehensive exam benchmark to date for the multilingual evaluation of
vision-language models. Kaleidoscope is a large-scale, in-language multimodal
benchmark designed to evaluate VLMs across diverse languages and visual inputs.
Kaleidoscope covers 18 languages and 14 different subjects, amounting to a
total of 20,911 multiple-choice questions. Built through an open science
collaboration with a diverse group of researchers worldwide, Kaleidoscope
ensures linguistic and cultural authenticity. We evaluate top-performing
multilingual vision-language models and find that they perform poorly on
low-resource languages and in complex multimodal scenarios. Our results
highlight the need for progress on culturally inclusive multimodal evaluation
frameworks.",http://arxiv.org/pdf/2504.07072,"['Israfel Salazar', 'Manuel Fernández Burda', 'Shayekh Bin Islam', 'Arshia Soltani Moakhar', 'Shivalika Singh', 'Fabian Farestam', 'Angelika Romanou', 'Danylo Boiko', 'Dipika Khullar', 'Mike Zhang', 'Dominik Krzemiński', 'Jekaterina Novikova', 'Luísa Shimabucoro', 'Joseph Marvin Imperial', 'Rishabh Maheshwary', 'Sharad Duwal', 'Alfonso Amayuelas', 'Swati Rajwal', 'Jebish Purbey', 'Ahmed Ruby', 'Nicholas Popovič', 'Marek Suppa', 'Azmine Toushik Wasi', 'Ram Mohan Rao Kadiyala', 'Olga Tsymboi', 'Maksim Kostritsya', 'Bardia Soltani Moakhar', 'Gabriel da Costa Merlin', 'Otávio Ferracioli Coletti', 'Maral Jabbari Shiviari', 'MohammadAmin farahani fard', 'Silvia Fernandez', 'María Grandury', 'Dmitry Abulkhanov', 'Drishti Sharma', 'Andre Guarnier De Mitri', 'Leticia Bossatto Marchezi', 'Johan Obando-Ceron', 'Nazar Kohut', 'Beyza Ermis', 'Desmond Elliott', 'Enzo Ferrante', 'Sara Hooker', 'Marzieh Fadaee']",2025-04-09 17:43:16+00:00,cs.CL,"Independent Variables: Languages, Visual Inputs

Dependent Variables: Performance of Vision-Language Models","Languages, Visual Inputs",Performance of Vision-Language Models
A Survey on Personalized and Pluralistic Preference Alignment in Large Language Models,"Personalized preference alignment for large language models (LLMs), the
process of tailoring LLMs to individual users' preferences, is an emerging
research direction spanning the area of NLP and personalization. In this
survey, we present an analysis of works on personalized alignment and modeling
for LLMs. We introduce a taxonomy of preference alignment techniques, including
training time, inference time, and additionally, user-modeling based methods.
We provide analysis and discussion on the strengths and limitations of each
group of techniques and then cover evaluation, benchmarks, as well as open
problems in the field.",http://arxiv.org/pdf/2504.07070,"['Zhouhang Xie', 'Junda Wu', 'Yiran Shen', 'Yu Xia', 'Xintong Li', 'Aaron Chang', 'Ryan Rossi', 'Sachin Kumar', 'Bodhisattwa Prasad Majumder', 'Jingbo Shang', 'Prithviraj Ammanabrolu', 'Julian McAuley']",2025-04-09 17:39:58+00:00,cs.CL,"Independent Variables: personalized preference alignment techniques, training time methods, inference time methods, user-modeling methods

Dependent Variables: user satisfaction, alignment of LLM behavior to dynamic user preferences","personalized preference alignment techniques, training time methods, inference time methods, user-modeling methods","user satisfaction, alignment of LLM behavior to dynamic user preferences"
